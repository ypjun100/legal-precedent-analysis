{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벌금 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 11:59:56.816355: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-12 11:59:56.879195: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-12 11:59:56.880544: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-12 11:59:57.936044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "seed_value= 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 가져오기 및 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel(\"./output/02-extract-nouns.xlsx\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataframe['extracted_nouns'], dataframe['fine'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저 정의 및 임베딩 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 30000\n",
    "sequence_length = 256\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, sequence_length)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, sequence_length)\n",
    "\n",
    "# 인퍼런스 환경에서 만들어진 토크나이저를 사용하기 위해 피클로 저장\n",
    "# import pickle\n",
    "\n",
    "# with open('./model/tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12673\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(\"./model/ft_1119.txt\",  encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "embedding_dim = 200\n",
    "num_filters = 100\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_3 = tf.keras.Input(dtype = tf.float32, shape = (sequence_length,))\n",
    "embedding_layer_3 = tf.keras.layers.Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=True)(input_3)\n",
    "\n",
    "reshape_3 = tf.keras.layers.Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)\n",
    "\n",
    "conv_0_3 = tf.keras.layers.Conv2D(num_filters, kernel_size=(3, embedding_dim), activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(3))(reshape_3)\n",
    "conv_1_3 = tf.keras.layers.Conv2D(num_filters, kernel_size=(4, embedding_dim), activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(3))(reshape_3)\n",
    "conv_2_3 = tf.keras.layers.Conv2D(num_filters, kernel_size=(5, embedding_dim), activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(3))(reshape_3)\n",
    "\n",
    "maxpool_0_3 = tf.keras.layers.MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)\n",
    "maxpool_1_3 = tf.keras.layers.MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)\n",
    "maxpool_2_3 = tf.keras.layers.MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)\n",
    "\n",
    "concatenated_tensor_3 = tf.keras.layers.Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3])\n",
    "flatten_3 = tf.keras.layers.Flatten()(concatenated_tensor_3)\n",
    "dropout_3 = tf.keras.layers.Dropout(rate = 0.5)(flatten_3)\n",
    "\n",
    "dense_layer_3 = tf.keras.layers.Dense(units = 256, activation = tf.nn.relu)(dropout_3)\n",
    "dense_layer_4 = tf.keras.layers.Dense(units = 64, activation = tf.nn.relu)(dense_layer_3)\n",
    "dense_layer_5 = tf.keras.layers.Dense(units = 16, activation = tf.nn.relu)(dense_layer_4)\n",
    "\n",
    "output = tf.keras.layers.Dense(units = 1, activation = tf.nn.relu)(dense_layer_5)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_3, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 256, 200)             2534600   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 256, 200, 1)          0         ['embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 254, 1, 100)          60100     ['reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 253, 1, 100)          80100     ['reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 252, 1, 100)          100100    ['reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 1, 1, 100)            0         ['conv2d_3[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPoolin  (None, 1, 1, 100)            0         ['conv2d_4[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPoolin  (None, 1, 1, 100)            0         ['conv2d_5[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " concatenate_1 (Concatenate  (None, 3, 1, 100)            0         ['max_pooling2d_3[0][0]',     \n",
      " )                                                                   'max_pooling2d_4[0][0]',     \n",
      "                                                                     'max_pooling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 300)                  0         ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 300)                  0         ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 256)                  77056     ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 64)                   16448     ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 16)                   1040      ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 1)                    17        ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2869461 (10.95 MB)\n",
      "Trainable params: 2869461 (10.95 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001), metrics=['mse', 'mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511/511 [==============================] - 64s 122ms/step - loss: 33548.2812 - mse: 33486.5352 - mae: 56.8382 - val_loss: 17900.6406 - val_mse: 17819.8555 - val_mae: 33.1356\n",
      "Epoch 2/10\n",
      "511/511 [==============================] - 66s 130ms/step - loss: 20872.1484 - mse: 20777.4355 - mae: 36.9701 - val_loss: 17927.0527 - val_mse: 17820.9336 - val_mae: 34.0390\n",
      "Epoch 3/10\n",
      "511/511 [==============================] - 70s 137ms/step - loss: 17353.1133 - mse: 17238.5469 - mae: 32.6153 - val_loss: 14343.9170 - val_mse: 14221.6895 - val_mae: 27.6930\n",
      "Epoch 4/10\n",
      "511/511 [==============================] - 63s 123ms/step - loss: 16324.5410 - mse: 16194.4863 - mae: 31.3220 - val_loss: 13605.2793 - val_mse: 13469.6758 - val_mae: 27.1279\n",
      "Epoch 5/10\n",
      "511/511 [==============================] - 63s 123ms/step - loss: 15295.9121 - mse: 15153.1582 - mae: 30.2819 - val_loss: 13356.4141 - val_mse: 13208.2119 - val_mae: 26.8016\n",
      "Epoch 6/10\n",
      "511/511 [==============================] - 63s 123ms/step - loss: 14446.2373 - mse: 14291.3809 - mae: 29.4958 - val_loss: 12847.3438 - val_mse: 12686.8740 - val_mae: 26.5677\n",
      "Epoch 7/10\n",
      "511/511 [==============================] - 62s 121ms/step - loss: 13965.2021 - mse: 13797.7197 - mae: 28.9647 - val_loss: 14115.5029 - val_mse: 13941.0146 - val_mae: 28.8994\n",
      "Epoch 8/10\n",
      "511/511 [==============================] - 64s 124ms/step - loss: 13606.5576 - mse: 13426.7441 - mae: 28.6176 - val_loss: 14368.5771 - val_mse: 14182.7354 - val_mae: 30.0345\n",
      "Epoch 9/10\n",
      "511/511 [==============================] - 64s 126ms/step - loss: 12600.2422 - mse: 12408.9912 - mae: 27.7752 - val_loss: 12599.8594 - val_mse: 12403.5371 - val_mae: 25.1913\n",
      "Epoch 10/10\n",
      "511/511 [==============================] - 70s 137ms/step - loss: 12500.7686 - mse: 12298.1992 - mae: 27.5675 - val_loss: 12336.9189 - val_mse: 12129.4062 - val_mae: 24.9268\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
    "history = model.fit(x=X_train, y=y_train, batch_size=32, epochs=100, verbose=1, validation_split=0.1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/fine_231122.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 모델 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 결과값이 회귀 형태로 도출되기 때문에 이를 분류 문제로 변경하기 위해 예측값과 실제값이 오차범위 내에 있다면 정답, 아니라면 오답이라 간주하여 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 19ms/step\n",
      "0.8929633300297324\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "from keras.models import load_model\n",
    "\n",
    "def calculate_accuracy(model, inputs, labels, error_torlerance=0.2):\n",
    "    y_hats = model.predict(inputs)\n",
    "    score = sum([True if abs(y_hat - labels[i]) <= error_torlerance else False for i, y_hat in enumerate(y_hats)])\n",
    "    print(score / len(inputs))\n",
    "\n",
    "model = load_model(\"./model/fine_231122.keras\")\n",
    "calculate_accuracy(model, X_test, y_test.to_numpy(), error_torlerance=5) # 예측과 정답 간 error_torlerance 차이는 정답이라 가정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
